{
  
    
        "post0": {
            "title": "Making my Personal Website Fast",
            "content": "Introduction . For a large segment of internet users, waiting for a page that may never fully load breaks the promise of the internet even to the point of putting lives at risk during emergencies. . A lot of energy is used to power data centers and the devices they serve. We can help reduce overall energy demands by keeping our file payloads smaller which also keeps payload transmission faster and more efficient. https://almanac.httparchive.org/en/2021/page-weight . The internet has become heavier and heavier over the years. Developers pulling in random libraries and frameworks and not caring about the end user needing to download all that data. We need to return to simplicity. . With this as my guiding principle, I set out to make my personal website as light as possible - without sacrificing on the esthetics. I will be sharing my journey and the tools I used to achieve this goal. . Disclaimer: This blog is not optimized in any way yet. I am no role model (yet). I am just taking small steps to make it better. . The journey . At the moment my website serves one purpose. It hosts my CV. My ambition is not to make it look flashy with a lot of moving content. I am fond of the idea of simple websites that focus on the content. Preferably, and as mine is at the moment, they should have no javascript at all. . My markdown website . I host my website on github pages. They provide a free hosting service for static websites with very fast load times. They have support for markdown websites. You just write everything in a file called index.md and github will render it for you. This is what I used this for a while, because it was super easy to get started. No need to worry about hosting, or setting up a build pipeline. I could just focus on writing. . The images that I am going to show below are form the “network tab” in the developer tools. They show the sequence of requests made to the content providers. . This is what my website looked like back then. . Initial As you can see there are a lot of request being made. Comparing it to page-weight statistics from the httparchive it wasn’t that heavy to begin with. The only issue is that there was not a lot of content on the website to justify the 1M size. This was mainly due to mermaid.js. This one library was about 200 times heavier than the actual content itself. It is a library used to convert mermaid.js diagrams to SVG on the client. . I noticed that the mermaid library that I was using was not minified. Looking around I managed to find a minified version of the library. This is what it looked like after minification. . Minified Mermaid . This managed to reduce the size of the library a bit, but there was still a lot of room for improvement. . This is when I decided to go for a more custom solution. . HTML and CSS (and SVG) . I realized that if I wanted to become fast I needed to ditch github’s markdown renderer. . The website was rewritten in HTML and CSS. I also decided to pre-render the mermaid diagrams to SVG. This way I could just include the SVG in the HTML and not have to worry about the client needing to download the whole library and rendering the diagram. . HTML and SVGs . This was the biggest improvement in performance that I made. Now the whole website weighted about 25kb. The size was reduced 50 fold. . During the conversion to HTML I pulled in Open Sans from google fonts to replace the one github uses for their markdown. Open Sans is the last request being made in the image above. This extended the load by about 25ms at best. Because first it loaded in a css file, which then downloaded the font. I tried first hosting the fonts together with the other files. Which removed the extra fetching of the css file, but now the font was many times heavier than before. As can be seen in the image below, the font request has a large portion of the bar that is blue, meaning that it is spending almost half the time downloading. . Local Font . I ended up styling a web-safe font to look like Open Sans. This way I could get rid of the extra request and can celebrate a boot in performance. . No font . The website size had decrease to 8kb! A 150 fold improvement from the start. However, the total load time was the same because there were still a lot of different requests being made. . At time point, the website is basically a html file that loads in two svg. A nice thing about SVGs is that they can be inlined. Meaning that everything can be bundled into a single file. This is what I did next. . Final (?) . After manually inlining the SVGs, the website is now a single html file! . The journey could end here. I was happy with the result. However, I wanted to see if I could do better. . Using Astro . Initially I did not want to use any frameworks. I wanted to keep it as close to vanilla as possible. My requirements were simple: . No: shipping any javascript if the website was’t interactive. | No: a runtime | Yes: Server side rendering (SSR) | Yes: reusable components without the need for javascript | Yes: markdown support (future blog?) | . This rules out the majority of javascript frameworks I knew of. However, Astro looked like a good fit. It is a static site generator that uses snowpack under the hood. It also has support for markdown and SSR. I decided to give it a try. . Converting the website from plain html to Astro was pretty straight forward. It was just a matter of factoring out the common parts into components and I was done. . A problem that I ran into was that building using Astro generated a directory that contained: . index.html | subdirectory with css files | SVG files | . This was not ideal, it would be going back to how it was before. To remedy this I added a script that at build time would inline all the css and svg files into the html file. A nice feature was that it also minified everything making the final index.html smaller! . Astro . The website was actually smaller after adding the framework (mostly thanks to the minifier). . Note, using Astro can result in more boiler plate in the final build if the developer is not careful. For example, generously using layout components that contain one or more divs can result in an exponential increase in the amount of divs in the final html. . Conclusion . We can safely say that any client with using a recent browser can load this website without any issues. Even time travelers with a 56k modem can load it in under one second. . By just taking a little bit of care and doing some optimizations it can make a huge difference in performance. This, without any impact on design, everything looks the same. . This website is a candidate for the 10kb club. .",
            "url": "https://nicolo.io/2023/03/12/making-a-fast-website.html",
            "relUrl": "/2023/03/12/making-a-fast-website.html",
            "date": " • Mar 12, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Recipes as a Service",
            "content": "The goal of this project is to create recipe recommendations that optimize the use of ingredients. By finding recipes that have a high number of ingredients in common we as consumers are able to decrease our food expenses while utilizing our groceries more effectively. Furthermore, by using the store&#39;s promotions we are able to reduce expenses even further and help the stores sell the items they wish to get rid of. . Background . I have always had a hard time at planning a week&#39;s meals. It is trivial to randomly pick 7 recipes from a list of recipes and create a shopping list by concatenating the recipes&#39; ingredients list. Although this approach results in a varied week, such a grocery list is extremely expensive! A second problem that I have have is the ineffective usage of all food items in the fridge. Having to throw out vegetables because of spoilage is a problem that could easily be avoided if all food items had a purpose. . This post . In this post we explore various ways of finding recipes that have ingredients in common. Although this task might seem simple at first, it turns out that the problem has a factorial time complexity. Given 1000 recipes, finding pairs of recipes with many ingredients in common only requires you to check 500 000 combinations. While finding a triad would have you checking 166 000 000 combinations. A week (5 days = 5 recipes) is 8.25e16 combinations. The time complexity of this problem is in the order of O(n!). Among time complexities, that is a tough one. . . As the introduction mentions, we do not only want to find recipes with ingredients in common, but also optimize for: . Total number of ingredients | Total price | Store promotions | . However, this will have to come later as right now I just want to create an MVP. . The algorithms in this post . Brute force (checking ingredients in common between r recipes) Standard | Caching intermediate results | Multiprocessing | . | Pick randomly Only pick, then pick a similar one | Weighted probability | . | Find groups of groups | . Stay tuned for the graphs at the end! . from tqdm import tqdm import json import itertools import matplotlib.pyplot as plt from math import comb import random . The dataset is around 30MB and contains approximately 1000 recipes. For this post, only the names of the recipes and the ingredient lists are needed. Further on, more information about the recipe is going to be needed. . with open(&quot;database.json&quot;, &quot;r&quot;) as f: data = json.load(f) recipes = {} for recipe in data: products = recipe[&quot;products&quot;].get(&quot;productsToBuy&quot;, []) ingredients = [product[&quot;product&quot;][&quot;name&quot;] for product in products] if len(ingredients) == 0: continue recipes[recipe[&quot;id&quot;]] = ingredients print(len(recipes), &quot;recipes&quot;) print(next(iter(recipes.items()))) . 1032 recipes (&#39;grillad_avokado&#39;, [&#39;Avokado Klass 1&#39;, &#39;Lime Klass 1&#39;]) . Helper functions . def in_common(recipes: list[list[str]]): &quot;&quot;&quot; Takes a list of lists of ingredients and returns the ingredients that are in common between all of them. &quot;&quot;&quot; return len(list(set.intersection(*map(set, recipes)))) def recipes2ingredients(names: list[str]): &quot;&quot;&quot; Takes a list of names and returns a list of lists of ingredients. &quot;&quot;&quot; return [recipes[name] for name in names] def print_week(week: tuple[str, int]): &quot;&quot;&quot; Takes a list of names and prints them in a nice way. Example: =======Recipes======= RecipeA RecipeB RecipeC =====Ingredients===== In common: 2 IngredientA IngredientB ===================== &quot;&quot;&quot; ingredients = recipes2ingredients(week[0]) num_ingredients = len(list(set(sum(ingredients, [])))) num_in_common = in_common(ingredients) return_str = [] return_str.append(&quot;=======Recipes=======&quot;) return_str.append(&quot; n&quot;.join(week[0])) return_str.append(&quot;=====Ingredients=====&quot;) return_str.append(&quot; &quot;.join([&quot;In common&quot;, str(num_in_common)])) return_str.append(&quot; &quot;.join([&quot;Num ingredients&quot;, str(num_ingredients)])) return_str.append(&quot; &quot;.join([&quot;Avg. ingredient reuse&quot;, str(round(ingredient_reuse_avg(week[0]), 2))])) return_str.append(&quot;&quot;) return_str.append(&quot; n&quot;.join(set.intersection( *map(set, ingredients)))) return_str.append(&quot;=====================&quot;) return &quot; n&quot;.join(return_str) def ingredient_reuse(ingredient, recipes_names): &quot;&quot;&quot; Takes an ingredient and a list of recipes and returns the number of times it is used in the recipes. &quot;&quot;&quot; return sum([ingredient in recipes2ingredients(recipe) for recipe in recipes_names]) def ingredient_reuse_avg(recipes_name): &quot;&quot;&quot; Takes a list of recipes and returns the average number of times an ingredient is used in a recipe. &quot;&quot;&quot; from collections import Counter ingredients = list(map(list, map(set, recipes2ingredients(recipes_name)))) # Remove duplicates ingredients in recipes all_ingredients = sum(ingredients, []) count = Counter(all_ingredients) return sum(count.values()) / len(count) . All combinations . First out is the brute force method. This one was tried out because of its simplicity, but it was quickly realized that it was unfeasible. For fun, two different modifications were made in order to speed up the computation. Alas, the whole approach would have to be rejected. . Standard . This approach works for m&lt;4. For higher values of m it is completely unusable. . num_recipes = 2 results = {} for recipe_comb in tqdm(itertools.combinations(recipes, num_recipes), total=comb(len(recipes), num_recipes)): if tuple(sorted(recipe_comb)) in results: continue ingredients = [recipes[recipe] for recipe in recipe_comb] results[tuple(sorted(recipe_comb))] = in_common(ingredients) results = sorted(results.items(), key=lambda x: x[1], reverse=True) . 100%|██████████| 531996/531996 [00:00&lt;00:00, 576031.92it/s] . Caching computations . Since taking the intersection of sets of ingredients is commutative (meaning that the order does not matter). Intersections can be cached and reused. . Example: . Recipes: a, b, c, d. . Ingredients lists: Ia, Ib, Ic, Id. . Ingredients in common between recipes a and b: Ia ∩ Ib = Iab . Ingredients in common between recipe a, b and c: . Ia ∩ Ib ∩ Ic = Iab ∩ Ic = Iabc(Iab is cached) . Later when we want to compute the intersection of a, b and d, the cached intersection Iab can be reused in Iab ∩ Id. . from functools import lru_cache # Convert the ingredient lists to sets of ingredients once recipes_sets = dict(map(lambda x: (x, set(recipes[x])), recipes)) @lru_cache(maxsize=None) def in_common_cache(recipes_i: tuple[str]): &quot;&quot;&quot; Cached in_common function. Takes a list of recipe names and returns the ingredients that are in common between all of them. If more than two recipes are passed, a recursive call is made. &quot;&quot;&quot; if len(recipes_i) &gt; 2: return set.intersection(recipes_sets[recipes_i[0]], in_common_cache(recipes_i[1:])) return set.intersection(recipes_sets[recipes_i[0]], recipes_sets[recipes_i[1]]) num_recipes = 2 results = {} total = comb(len(recipes), num_recipes) for i, recipe_comb in tqdm(enumerate(itertools.combinations(recipes, num_recipes)), total=total): sorted_recipe_comb = tuple(sorted(recipe_comb)) if sorted_recipe_comb in results: continue results[sorted_recipe_comb] = len(in_common_cache(sorted_recipe_comb)) if i % (total//20) == 0: print(in_common_cache.cache_info()) results = sorted(results.items(), key=lambda x: x[1], reverse=True) . It turns out that the cached search is not very effective in practice. Perhaps the implementation is lacking. Its performance is the same or worse depending on the maxsize used. . Multiprocessing . This approach splits the work on multiple cores in order to parallelize the computation. It simply runs the brute force approach on multiple processes. . One caveat of running multiprocessing in jupyter notebook is that the map function needs to be defined outside the notebook. Here is that function: . def pool_func(recipes): return (recipes, in_common(recipes2ingredients(recipes))) . import multiprocessing import defs num_recipes = 2 if __name__ == &quot;__main__&quot;: with multiprocessing.Pool(processes=8) as pool: combinations = list(itertools.combinations(recipes, num_recipes)) it = pool.map(defs.pool_func, itertools.combinations(recipes, num_recipes)) # result = list(tqdm(it, total=comb(len(recipes), num_recipes))) . for id, week in enumerate(results[:3]): print(print_week(week), &quot; n n&quot;) . =======Recipes======= tacos_pescado tomatillo_chipotle =====Ingredients===== In common 9 Num ingredients 12 Avg. ingredient reuse 1.75 Avokado Klass 1 Jalapeño Klass 1 Salladslök Knippe Chipotle i Adobosås Lök Vit Morado Koriander i Kruka Ekologisk Klass 1 Lök Röd Klass 1 Tomater Hela Ekologiska Lime Klass 1 ===================== =======Recipes======= saffransbrod saffranskrans_apelsin =====Ingredients===== In common 9 Num ingredients 13 Avg. ingredient reuse 1.69 Apelsin Klass 1 Mandelmassa Strösocker Russin Eko Kärnfria Ägg 6p Frigående L Eko Jäst Sockerdeg Vetemjöl Special Mjölk Standard 3% Saffran Påse ===================== =======Recipes======= citron_hallonbulle kanelnystan =====Ingredients===== In common 8 Num ingredients 13 Avg. ingredient reuse 1.62 Kardemumma Malen Strösocker Mandelmassa Vetemjöl Ekologiskt Kronjäst Färsk Ägg 6p Frigående L Eko Smör Normalsaltat Mjölk Standard 3% ===================== . Strangely enough, iPython is not able to utilize multiple cores to 100% and instead has multiple processes running at (100/n_processes)%. There might be some way to better utilize the cores, but it is probably not worth pursuing further due to the scaling issues mentioned earlier. . Pick one and then find a similar one . Having realized that the brute force approached probably will never work (especially for greater values of m), I started looking at alternative algorithms. The first one that came to mind was a stochastic one. It basically boils down to the following steps: . Choose and remove a recipe from the list of all recipes. | Compute the ingredients in common with all other recipes. | Choose and remove a recipe among the ones with the most ingredients in common. | Repeat step 2 and 3 until m recipes have been chosen. Store the names of the recipes and ingredients in common in a dict. | Repeat step 1 - 4 until enough samples have been taken. | Sort groups of recipes by most ingredients in common. | num_recipes = 4 results = {} for _ in tqdm(range(5000)): recipes_left = recipes.copy() # Make a copy of all the recipes so that we can remove them from the list week = [random.choice(list(recipes_left.keys()))] # Choose a random start-recipe recipes_left.pop(week[0]) # Remove the start-recipe from the list for _ in range(num_recipes-1): current_ingredients = sum([recipes[recipe] for recipe in week], []) # make a list with all the current ingredients in the week similarity = {name: in_common([current_ingredients, recipes[name]]) for name in recipes_left} # calculate the similarity between the current week and all the remaining recipes sorted_similarity = sorted(similarity.items(), key=lambda x: x[1], reverse=True) # sort the similarity by the highest similarity top_score = sorted_similarity[0][1] # get the highest similarity score top_recipes = [name for name, score in sorted_similarity if score == top_score] # keep only the recipes with the highest similarity week.append(random.choice(top_recipes)) # add a random recipe from the top-similarity recipes to the week recipes_left.pop(week[-1]) # remove the recipe from the list of remaining recipes if current_ingredients==[]: continue # Some recipes have no ingredients current_ingredients = recipes2ingredients(week) # make a list of lists of ingredients score = in_common(current_ingredients) results[tuple(sorted(week))] = score results = sorted(results.items(), key=lambda x: x[1], reverse=True) P1P = results # Save the results for later print(next(iter(results))) . 100%|██████████| 5000/5000 [00:18&lt;00:00, 265.50it/s] . ((&#39;citron_hallonbulle&#39;, &#39;favorit_citrussemla&#39;, &#39;kanelnystan&#39;, &#39;minisemlor&#39;), 5) . . for id, week in enumerate(results[:3]): print(print_week(week), &quot; n n&quot;) . =======Recipes======= citron_hallonbulle favorit_citrussemla kanelnystan minisemlor =====Ingredients===== In common 5 Num ingredients 19 Avg. ingredient reuse 2.16 Mandelmassa Kronjäst Färsk Ägg 6p Frigående L Eko Smör Normalsaltat Mjölk Standard 3% ===================== =======Recipes======= happy_fish hummercocktail_med_krisp pulled_pork_tacos_med_guacamole tacos_med_lax_och_avokadokram =====Ingredients===== In common 5 Num ingredients 26 Avg. ingredient reuse 1.65 Peppar Röd Klass 1 Tortilla Orginal Medium Koriander i Kruka Ekologisk Klass 1 Avokado Klass 1 Lime Klass 1 ===================== =======Recipes======= grillad_majs_tomatsalsa pork_carnitas tacos_pescado tomatillo_chipotle =====Ingredients===== In common 5 Num ingredients 21 Avg. ingredient reuse 1.95 Jalapeño Klass 1 Koriander i Kruka Ekologisk Klass 1 Lök Röd Klass 1 Tomater Hela Ekologiska Lime Klass 1 ===================== . This method is quite fast and generated decent results. Instead of scaling by O(m!), it seems to scale by O(m). . One possible modification to this algorithm is to make the likelihood of a recipe being picked be based on how similar it is to the initial recipe. This is partly inspired by how breeding in genetic algorithms. . results = {} for _ in tqdm(range(5000)): recipes_left = recipes.copy() # Make a copy of all the recipes so that we can remove them from the list week = [random.choice(list(recipes_left.keys()))] # Choose a random start-recipe recipes_left.pop(week[0]) # Remove the start-recipe from the list for _ in range(3): current_ingredients = sum([recipes[recipe] for recipe in week], []) # make a list with all the current ingredients in the week similarity = {name: in_common([current_ingredients, recipes[name]]) for name in recipes_left} # calculate the similarity between the current week and all the remaining recipes sorted_similarity = sorted(similarity.items(), key=lambda x: x[1], reverse=True) sorted_similarity = list(filter(lambda r: r[1]&gt;0, sorted_similarity)) # Remove recipes with zero similarity sorted_similarity = sorted_similarity[:100] # Keep only top 100 recipe_names, scores = list(map(list, zip(*sorted_similarity))) picked = random.choices(recipe_names, weights=scores) # pick a random recipe from the similarity list with weights week.append(picked[0]) recipes_left.pop(week[-1]) # remove the recipe from the list of remaining recipes if current_ingredients==[]: continue # Some recipes have no ingredients current_ingredients = recipes2ingredients(week) # make a list of lists of ingredients score = in_common(current_ingredients) results[tuple(sorted(week))] = score results = sorted(results.items(), key=lambda x: x[1], reverse=True) P1PW = results # Save the results for later print(next(iter(results))) . 100%|██████████| 5000/5000 [00:18&lt;00:00, 263.28it/s] . ((&#39;enkel_gronsakstagine&#39;, &#39;glogg_revben&#39;, &#39;kottfarssas_beluga&#39;, &#39;spaghetti_norma&#39;), 4) . . The results were not as good as expected. This is probably because there isn&#39;t a huge difference between the worse performing (similarity=0) and a good performer (similarity=4). . Find groups of recipes . A suggestion made by a colleague was to find groups of groups of recipes. Not only should this result in fewer combinations needed to be checked, but it might actually result in better reuse of ingredients (see conclusion). . num_recipes = 2 results = {} for recipe_comb in tqdm(itertools.combinations(recipes, num_recipes), total=comb(len(recipes), num_recipes)): if tuple(sorted(recipe_comb)) in results: continue ingredients = [recipes[recipe] for recipe in recipe_comb] results[tuple(sorted(recipe_comb))] = in_common(ingredients) results = sorted(results.items(), key=lambda x: x[1], reverse=True) scores = results[:5000] # Only keep the top 10% of the results scores[0] . 100%|██████████| 531996/531996 [00:00&lt;00:00, 584116.16it/s] . ((&#39;tacos_pescado&#39;, &#39;tomatillo_chipotle&#39;), 9) . results = {} pairs = [pair[0] for pair in scores] for recipe_comb in tqdm(itertools.combinations(pairs, num_recipes), total=comb(len(pairs), num_recipes)): if len(set.intersection(*map(set, recipe_comb))) &gt; 0: continue if tuple(sorted(recipe_comb)) in results: continue ingredients = map(lambda r: set( sum(recipes2ingredients(r), [])), recipe_comb) ingredients = map(set, ingredients) ingredients = list(map(list, ingredients)) results[tuple(sorted(recipe_comb))] = in_common(ingredients) results = sorted(results.items(), key=lambda x: x[1], reverse=True) GG = results results[0] . 100%|██████████| 12497500/12497500 [01:15&lt;00:00, 165342.30it/s] . (((&#39;fargsprakande_nudelsallad&#39;, &#39;glasnudelsallad_halloumi&#39;), (&#39;mk38_asiatiska_kottbullar&#39;, &#39;vietnamesiska_varrullar&#39;)), 15) . for id, week in enumerate(results[:3]): week_recipes = sum(map(list, week[0]), []) week = (week_recipes, week[1]) print(print_week(week), &quot; n n&quot;) . =======Recipes======= fargsprakande_nudelsallad glasnudelsallad_halloumi mk38_asiatiska_kottbullar vietnamesiska_varrullar =====Ingredients===== In common 3 Num ingredients 38 Avg. ingredient reuse 1.61 Morot Klass 1 Lök Vit Morado Lime Klass 1 ===================== =======Recipes======= glasnudelsallad_halloumi krabbpasta_singapore mk38_asiatiska_kottbullar vietnamesiska_varrullar =====Ingredients===== In common 2 Num ingredients 35 Avg. ingredient reuse 1.63 Lök Vit Morado Lime Klass 1 ===================== =======Recipes======= glasnudelsallad_halloumi som22_somriga_thaitacos mk38_asiatiska_kottbullar vietnamesiska_varrullar =====Ingredients===== In common 2 Num ingredients 37 Avg. ingredient reuse 1.57 Lök Vit Morado Lime Klass 1 ===================== . Let&#39;s graph the results! . It is pretty clear that the winners are Pick one random, then pick a similar one and the Pick groups, then pick groups of groups. Let&#39;s plot these results to see how well they share ingredients. . import networkx as nx def graph_recipes(recipe_names: list[str], ax=None): G = nx.Graph() G.add_nodes_from(recipe_names) for recipe_pair in itertools.combinations(recipe_names, 2): share = in_common(recipes2ingredients(recipe_pair)) if share &gt; 0: G.add_edge(*recipe_pair, weight=share) pos = nx.spring_layout(G, seed=7) labels = nx.get_edge_attributes(G, &#39;weight&#39;) weights = list(labels.values()) if ax: nx.draw_networkx(G, pos, ax=ax) nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, ax=ax, font_size=20, rotate=False) nx.draw_networkx_edges(G, pos, edgelist=G.edges(), width=weights, alpha=0.5, ax=ax) else: nx.draw_networkx(G, pos) nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, font_weight=40) nx.draw_networkx_edges(G, pos, edgelist=G.edges(), width=weights, alpha=0.5) fig = plt.figure(1, figsize=(10, 10), facecolor=&#39;w&#39;) fig.suptitle(&quot;Pick one, then pick a similar one&quot;, fontsize=16) fig, ax = plt.subplots(2, 2, num=1) for i, P1P_week in enumerate(P1P[:4]): graph_recipes(P1P_week[0], ax=ax[i // 2, i % 2]) plt.show() fig = plt.figure(1, figsize=(10, 10), facecolor=&#39;w&#39;) fig.suptitle(&quot;Pick one, then pick a similar one WEIGHTED&quot;, fontsize=16) fig, ax = plt.subplots(2, 2, num=1) for i, P1PW_week in enumerate(P1PW[:4]): graph_recipes(P1PW_week[0], ax=ax[i // 2, i % 2]) plt.show() fig = plt.figure(1, figsize=(10, 10), facecolor=&#39;w&#39;) fig.suptitle(&quot;Groups of Groups&quot;, fontsize=16) fig, ax = plt.subplots(2, 2, num=1) for i, GG_week in enumerate(GG[:4]): week_recipes = sum(map(list, GG_week[0]), []) GG_week = (week_recipes, GG_week[1]) graph_recipes(GG_week[0], ax=ax[i // 2, i % 2]) plt.show() . Results . The results show that the winning methods are pick one, then pick a similar one and groups of groups. The former seems to have greater average shared ingredients. While the later has some large edges inside the subgroups and decently strong inter-group connections. The weighted approach was quite unsuccessful having a comparatively low average shared ingredients. . Conclusion . This notebook looked solely at different algorithm to find ingredients in common among recipes. There are many other metrics that could be used in order to find recipes that share ingredients. For example, instead of only looking at ingredients shared among all recipes, it might be better to use average number of shared ingredients between a pair of recipes. This might result in the finding of groups of recipes that share more total ingredients. Furthermore, subgroups of highly similar recipes can make the week more interesting. For example, eating only fish and potato based recipes 5 days in a row is probably very boring. While eating sandwiches three days and salad two days in an alternating fashion, is probably more interesting and has a higher number of shared ingredients. This is basically the groups of groups approach. . The only problem with the group of groups approach is that it is slow as it uses the brute force method to find small groups. This was mostly done for algorithm simplicity, but could easily be changed to a pick one, then pick a similar one approach. . Further research will be conducted on methods for finding shared ingredients using other metrics. .",
            "url": "https://nicolo.io/computer%20science/data%20science/2022/07/04/Recipes-as-a-Service-1.html",
            "relUrl": "/computer%20science/data%20science/2022/07/04/Recipes-as-a-Service-1.html",
            "date": " • Jul 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Writing a LLVM compiler using Python",
            "content": "Recently I have been fascinated by the world of compiled programming languages. My current day to day work mainly involves interpreted languages such as Python and TypeScript. These languages are great for productivity, but they leave a lot to be desired when it comes to performance and resource usage (see benchmarks). In order to learn more about compiled languages I started writing small programs in C++, go and Rust. Rust is awesome! In 13 lines of code a rust noob can managed to write a function that looks for a string (in lowercase and uppercase) inside of a SHA256 hash. The kicker, it is fully parallel thanks to the rayon library! This is usually not that easy to do in other system languages. The only difference between the single threaded and parallel code is .into_par_iter() part. . pub fn find_hash(name: &amp;str) -&gt; String { let name_upper = &amp;name.to_uppercase(); let result = (0u64..u64::MAX).into_par_iter().find_any(|&amp;i| { let mut hasher = Sha256::new(); hasher.update(i.to_string().as_bytes()); let hash = base64::encode(&amp;hasher.finalize()); return hash.contains(name) || hash.contains(name_upper); }); let mut hasher = Sha256::new(); hasher.update(result.unwrap().to_string().as_bytes()); let hash = base64::encode(&amp;hasher.finalize()); hash } . There is something that Rust has in common with that this post is about, it compiles down to LLVM intermediate representation (IR). LLVM is basically a set of tools for developers to make compilers. A developer just needs to get their code into IR. After that, LLVM can optimize the IR for various metrics and then either run it in a jitted execution engine or compile it to machine code. One other nice thing about IR is that it is &quot;portable&quot; (we&#39;ll come to why I wrapped portable in quotes). There is no need to rewrite your compiler for every system architecture. . Brainfuck . In order to learn LLVM I decided to implement a compiler for one of the simplest programming languages: brainfuck(BF). It is a tape based language that only has these operators: + - &lt; &gt; [ ] . ,. When a BF program starts an array of 8 bit integers is initialized with zeros, this is the tape. There is also a pointer that we&#39;ll call tape_ptr that points to the &quot;current&quot; element in the array. A program might look something like this: +++.. This means increment the integer at the tape_ptr by one three times, then write the integer at the tape_ptr to the standard output (in our case the console). One would think that a three would be printed, but the console interprets the integer as a char type. A char of value “3” does not display. If we wanted a three to print we would have to write -[--&gt;+&lt;]&gt;.. This program decrements the current element (which overflows into 255). Then it enters a sort of while-loop given by []. The code between the brackets is looped &quot;while the current element is not zero&quot;. The &gt; and &lt; shift the position of the tape_ptr either to the right or left. The program therefore does the following: . - decrement the current position | [--&gt;+&lt;] -- decrement the current element 5 times | &gt; move one step to the right | + increment the current element | &lt; move on step to the left (the initial position) | ] is the value at the current element zero? if TRUE continue to the next operation (to the right) | if FALSE go back to the previous [ | . | . | . write the current value to stdout | . The result is that the element in the second position is incremented once for every time the element in the first position is decremented five times. This gives 255/5=51, where 255 is the overflown integer and 51 is the ascii character &#39;3&#39;. We will ignore the , instruction as it is quite useless; I have personally never used it. It is used for reading from stdin. However, if one is interested in writing programs in BF one should implement it in order to be able to use the language fully! . Now to the compiler . In unrelated project have used a python library called Numba a lot to speed up numerical computations. It manages to be so fast by compiling a subset of python down to IR which is then run on LLVM&#39;s jit execution engine. It can easily speed up python by 24 times and come close to C++ speeds. Numba uses a library called llvmlite to generate the IR. This is the library we are going to be using. . This whole project has really been a brainfuck. The documentation has been much thinner than I am used and, in some cases, nonexistent. To learn how to make system calls to the operating system kernel in order to write to stdout I had to read what registers to populate with what values by looking at source code. Because we are doing this on... Darwin ARM64! Which wants to have values in other registers than the linux on ARM64, or so it seems depending on what documentation you read. At this level, the error messages stop being helpful. When trying to implement the syscalls the kernel would just say things like: . zsh:invalid system call ./a.out Ok thanks, very useful! . Enough, let&#39;s start coding: . from llvmlite import ir code=&quot;&quot; counter = 0 def block_namer(): global counter counter += 1 return &quot;block_%d&quot; % (counter-1) TAPE_LEN = 10 mod = ir.Module(&quot;MainModule&quot;) mod.triple = &quot;arm64-apple-macosx12.0.0&quot; lfunc = ir.Function(mod, ir.FunctionType(ir.IntType(8), []), &quot;main&quot;) entry_block = lfunc.append_basic_block(&#39;entry&#39;) builder = ir.IRBuilder(entry_block) exit_block = builder.append_basic_block(&quot;exit&quot;) mod . ; ModuleID = &#34;MainModule&#34; target triple = &#34;arm64-apple-macosx12.0.0&#34; target datalayout = &#34;&#34; define i8 @&#34;main&#34;() { entry: exit: } . The compiler starts with these lines. It defines a main function where, like many languages, is where the program starts executing. More specifically, in the entry code block. We also create an exit block that we will later use to exit the program. . EDIT: I forgot to mention that the code generated by the python block is the LLVM IR. This is the code that will later be compiled to machine code by clang. . Now let’s create our data structures. . tape = builder.alloca(ir.ArrayType(ir.IntType(8), TAPE_LEN)) builder.store(ir.Constant(ir.ArrayType(ir.IntType(8), TAPE_LEN), [0] * TAPE_LEN), tape) # Create tape pointer tape_ptr = builder.gep( tape, [ir.Constant(ir.IntType(8), 0), ir.Constant(ir.IntType(8), 0)]) mod . ; ModuleID = &#34;MainModule&#34; target triple = &#34;arm64-apple-macosx12.0.0&#34; target datalayout = &#34;&#34; define i8 @&#34;main&#34;() { entry: %&#34;.2&#34; = alloca [10 x i8] store [10 x i8] [i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0], [10 x i8]* %&#34;.2&#34; %&#34;.4&#34; = getelementptr [10 x i8], [10 x i8]* %&#34;.2&#34;, i8 0, i8 0 exit: } . We start by allocating an array of 8-bit integers of the length TAPE_LEN on the stack. This comes out to %&quot;.2&quot; = alloca [10 x i8]. The variable that is returned %.2 is a pointer to the array. Notice @ and %, these indicate that the variable is either global or local. For example, the function @&quot;main&quot; is global, but the pointer to the array %&quot;.2&quot; is local. . The next instruction initializes the array with zeroes. Notice the [10 x i8]* %&quot;.2&quot;, it refers to the pointer we created earlier as it is there we are storing the zeroes. LLVM IR is strictly typed, and the types are mentioned every time a variable is used. It says that %.2 is a pointer to an array of ten eight-bit integers. . In the following instruction we create a pointer to the first element of the array and store it in %.4 . In order to handle the while loops in the language we use LLVM IR&#39;s blocks. They work like labels that you can conditionally or unconditionally branch to. We keep track of the branches using a python stack (just a regular list). One issue I was having when I first started writing this compiler is that I was doing too much computation in python. I was basically precomputing the whole BF program. This is usually allowed (and recommended) for performance reasons. But then, why even write a compiler? So, the branching had to be done in BF. . blocks = [builder.append_basic_block(block_namer())] builder.branch(blocks[0]) builder = ir.IRBuilder(blocks[0]) mod . ; ModuleID = &#34;MainModule&#34; target triple = &#34;arm64-apple-macosx12.0.0&#34; target datalayout = &#34;&#34; define i8 @&#34;main&#34;() { entry: %&#34;.2&#34; = alloca [10 x i8] store [10 x i8] [i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0], [10 x i8]* %&#34;.2&#34; %&#34;.4&#34; = getelementptr [10 x i8], [10 x i8]* %&#34;.2&#34;, i8 0, i8 0 br label %&#34;block_0&#34; exit: block_0: } . The following code takes a BF instructions and translates them into the corresponding IR instructions. . for op in code: if op == &quot;+&quot;: val_at_ptr = builder.load(tape_ptr) builder.store(builder.add( val_at_ptr, ir.Constant(ir.IntType(8), 1)), tape_ptr) elif op == &quot;-&quot;: val_at_ptr = builder.load(tape_ptr) builder.store(builder.sub( val_at_ptr, ir.Constant(ir.IntType(8), 1)), tape_ptr) elif op == &quot;&gt;&quot;: tape_ptr = builder.gep(tape_ptr, [ir.Constant(ir.IntType(8), 1)]) elif op == &quot;&lt;&quot;: tape_ptr = builder.gep(tape_ptr, [ir.Constant(ir.IntType(8), -1)]) elif op == &quot;.&quot;: # https://go.dev/src/syscall/zsysnum_darwin_arm64.go fty = ir.FunctionType(ir.IntType(32), [ ir.IntType(32), # x16 (=4) ir.IntType(32), # x0 (=1) ir.IntType(8).as_pointer(), ir.IntType(32) ]) # Uncomment this to make char=3 =&gt; &quot;3&quot; # char = builder.add(char, ir.Constant(ir.IntType(8), 48)) builder.asm(fty, &quot;svc 0&quot;, &quot;=r,{x16},{x0},{x1},{x2}&quot;, ( ir.IntType(32)(4), ir.IntType(32)(1), tape_ptr, ir.IntType(32)(1) ), True, name=&quot;print&quot;) elif op == &quot;e&quot;: builder.asm(ir.FunctionType(ir.IntType(32), [ir.IntType(32), ir.IntType(32)]), &quot;svc 0&quot;, &quot;=r,{x0},{x16}&quot;, [ir.IntType(32)(8), ir.IntType(32)(1)], True, name=&quot;asm_add&quot;) elif op == &quot;[&quot;: # Create a new block # Make the current block branch to the new block blocks.append(builder.append_basic_block(block_namer()+&quot;_open&quot;)) builder.branch(blocks[-1]) builder = ir.IRBuilder(blocks[-1]) elif op == &quot;]&quot;: # Create a new block # Make the current block branch to the new block if the value at the current pointer is 0 val_at_ptr = builder.load(tape_ptr) branch_condition = builder.icmp_signed( &#39;==&#39;, val_at_ptr, ir.Constant(ir.IntType(8), 0)) open_block = blocks.pop() close_block = builder.append_basic_block( open_block.name.replace(&quot;open&quot;, &quot;close&quot;)) builder.cbranch(branch_condition, close_block, open_block) builder = ir.IRBuilder(close_block) . Increment &amp; Decrement . There is a lot of code to unpack here. Let’s start with the + and - instructions. . val_at_ptr = builder.load(tape_ptr) builder.store(builder.add( val_at_ptr, ir.Constant(ir.IntType(8), 1)), tape_ptr) . The IR will look something like this: . %&quot;.6&quot; = load i8, i8* %&quot;.4&quot; %&quot;.7&quot; = add i8 %&quot;.6&quot;, 1 store i8 %&quot;.7&quot;, i8* %&quot;.4&quot; . Remember that the pointer to the current element was .4 It loads the value of the current element into .6, then add 1 to it and finally stores it where .4 points. . Move tape pointer . tape_ptr = builder.gep(tape_ptr, [ir.Constant(ir.IntType(8), 1)]) . To move the tape pointer, we simply use the get element pointer function. This is similar to incrementing a pointer in regular C, but the function handles all the logistics with how many bytes to jump. . Loops . For [: . blocks.append(builder.append_basic_block(block_namer()+&quot;_open&quot;)) builder.branch(blocks[-1]) builder = ir.IRBuilder(blocks[-1]) . First, we create a new block and add it to the list of blocks. We make the current block branch to the new block unconditionally. This is because LLVM IR does not simply move to the next block after it has finished the instructions is the current block. It must explicitly be told to move to the block below (meh). . For ]: . val_at_ptr = builder.load(tape_ptr) branch_condition = builder.icmp_signed(&#39;==&#39;, val_at_ptr, ir.Constant(ir.IntType(8), 0)) open_block = blocks.pop() close_block = builder.append_basic_block(open_block.name.replace(&quot;open&quot;, &quot;close&quot;)) builder.cbranch(branch_condition, close_block, open_block) builder = ir.IRBuilder(close_block) . The logic is quite similar. But in this case whether we move to the next block depends on whether the value at the current pointer is zero when we are at the end of the loop. . For a program like this +[&gt;+[&gt;+&lt;-]&lt;-]&gt;&gt; the branching will look like this. . define i8 @&quot;main&quot;() { entry: ... br label %&quot;block_0&quot; exit: ... ret i8 %&quot;.35&quot; block_0: ... br label %&quot;block_1_open&quot; block_1_open: ... br label %&quot;block_2_open&quot; block_2_open: ... br i1 %&quot;.24&quot;, label %&quot;block_2_close&quot;, label %&quot;block_2_open&quot; &lt;-- This is the end of the inner loop block_2_close: ... br i1 %&quot;.31&quot;, label %&quot;block_1_close&quot;, label %&quot;block_1_open&quot; &lt;-- This is the end of the outer loop block_1_close: .. br label %&quot;exit&quot; . Now to the hard part... &#128128; . We want to be able to print the current element&#39;s value to stdout. In order to do that the program has to talk to the kernel though a syscall. Think of it as calling on the operating system’s API that does things like write or read to stdout or files, open sockets, mount and unmount drives, change the permissions of files and much much more. The problem is that LLVM has nothing to do with the kernel. This is usually handled by local libraries. We therefore must implement this by writing native assembly. This is the part where the program becomes system architecture specific. I think that this is where we would normally be linking to local libraries that would handle this for us. . Since I am writing this on a ARM64, that is the syscall table I have to look at. As I am writing this, I wanted to add a link to the syscall table for Darwin ARM64, but I cannot find any. During the development process I scoured various forums, blogs and source code files on github to piece together how to make the correct syscall. This might be the best table I have found. However, it does not fully work on my machine. Depending on who you ask, Darwin ARM64 either uses register X16 or X8 to specify what syscall to make and uses either the value 0x40 or 4 or 0x2000004 to specify that we want to write. I have no idea, I must have tried all combinations of registers and values. . Making the syscall . fty = ir.FunctionType(ir.IntType(32), [ ir.IntType(32), # x16 (=4) ir.IntType(32), # x0 (=1) ir.IntType(8).as_pointer(), # x1 ir.IntType(32) # x2 ]) builder.asm(fty, &quot;svc 0&quot;, &quot;=r,{x16},{x0},{x1},{x2}&quot;, ( ir.IntType(32)(4), ir.IntType(32)(1), tape_ptr, ir.IntType(32)(1) ), True, name=&quot;print&quot;) . First, we create a function type, it is basically a call signature. The assembly will return a 32-bit integer and take the following inputs. The &quot;inputs&quot; are values that we are going to put in the registers {x16},{x0},{x1},{x2}. . 32-bit integer - The type of syscall (write call) | 32-bit integer - Where to write (stdout) | 8-bit integer - Pointer to the char array (in our case only the current element) | 32-bit integer - The length of the char array | Luckily, the llvmlite library has a convenient way to write out values to the registers automatically without us having to move every value into the registers. We just specify the &quot;constraint&quot; &quot;=r,{x16},{x0},{x1},{x2}&quot; and then pass the values in the args parameter . ir.IntType(32)(4), ir.IntType(32)(1), tape_ptr, ir.IntType(32)(1) . The actual assembly is the svc 0 instruction. . I also implemented an exit instruction to BF that can be used by writing e. It makes an exit syscall. . builder.asm(ir.FunctionType(ir.IntType(32), [ir.IntType(32), ir.IntType(32)]), &quot;svc 0&quot;, &quot;=r,{x0},{x16}&quot;, [ir.IntType(32)(8), ir.IntType(32)(1)], True, name=&quot;exit&quot;) . And that was pretty much it! The full code can be found in on my github. I hope you enjoyed this post! .",
            "url": "https://nicolo.io/computer%20science/lowlevel/compilers/2022/02/06/Brainfuck-LLVM-compiler.html",
            "relUrl": "/computer%20science/lowlevel/compilers/2022/02/06/Brainfuck-LLVM-compiler.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Building a Blockchain from scratch",
            "content": "What will be covered . My goals | What is a blockchain | Building a blockchain from scratch | My implementation | Future development | Appendix A | . Goals . My goal with this post and the code I have and will be writing is to get a solid understanding of how blockchains and crypto currencies work from the ground up. There is no better way to learn something that by doing it yourself. Firstly, I will implement a blockchain (this post), thereafter I will use the blockchain to build a distributed ledger and finally I shall build a crypto currency. . What is a blockchain . A blockchain is essentially just a linked list with extra features. These features make it hard to tamper with the data and easy to check the validity of the data. People use blockchains for many things. Most popularly for crypto currencies. Since it is anonymous individuals that manage the creation and validation of transactions. One wouldn&#39;t want someone to go back and change their account balance in order to make them rich. The currency is safe for as long as most individuals are honest. . Implementing a Blockchain . We will now implement a blockchain from scratch. To illustrate this, we will first implement a simple linked list and then build upon it until it becomes a blockchain. . Linked List . As the name suggests a linked list is a list where the data is linked together. An array on the other hand is data in memory stored in adjacent memory addresses. Note that the usual term for the object that stores the data in a linked list is node, however in blockchains the term is block. . Normally one would implement a singly linked list like this: . class Node: next_node = None def __init__(self, data:str=None): self.data = data class LinkedList: head=None def add(self, data:str): if self.head: current_node = self.head while current_node.next_node: current_node = current_node.next_node current_node.next_node = Node(data) else: # The first node gets an empty string as prevous hash because there are not previous nodes self.head = Node(data) . The problem with a simple linked list is that you can change the data anywhere in the list. Like so: . linked_list = LinkedList() for n in range(10): linked_list.add(str(n)) # Change the value of the third node linked_list.head.next_node.next_node.data = 12897 . Adding the hash of the previous block . We do not want anyone to change the data, so we add the hash of the previous node to the next node&#39;s fields. Now to check if the data in a node has been tampered with, we just recompute the hash of the previous node and compare with the current nodes &quot;hash of the previous block&quot;. . A quick refresher about hashes. A hash is the output of a hash function. Sometimes referred to as a fingerprint of some piece of data. A hash function is a so called &quot;one way&quot; function. You can easily compute the hash of a piece of data, but it is very hard to go from the hash to the original piece of data. The only way is to use &quot;brute-force&quot; and try all different combinations of inputs. If the original piece of data is longer than the hash, it is impossible since information has been lost. As shown in the example below, just a small change in the input creates a completely different output. Despite this, the function is non probabilistic, which means that a input will always have the same output. . from hashlib import sha256 def hasher(input: str): return sha256(input.encode()).hexdigest() print(hasher(&quot;Hello World&quot;)) print(hasher(&quot;Hello World!&quot;)) . a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e 7f83b1657ff1fc53b92dc18148a1d65dfc2d4b1fa3d677284addd200126d9069 . To add the hash of the previous block we add the field previous_node_hash to the node. . class Node: next_node = None def __init__(self, previous_node_hash:str, data:str=None): self.data = data self.previous_node_hash = previous_node_hash class LinkedList: head=None def add(self, data:str): if self.head: current_node = self.head while current_node.next_node: current_node = current_node.next_node current_node_hash = sha256(str(current_node.__dir__).encode()).hexdigest() # &lt;-- current_node.next_node = Node(current_node_hash, data) else: self.head = Node(&quot;&quot;, data) . A node now looks like this when printed . linked_list = LinkedList() for n in range(10): linked_list.add(str(n)) print(linked_list.head.next_node.__dict__) . {&#39;data&#39;: &#39;1&#39;, &#39;previous_node_hash&#39;: &#39;0e69ce586d540294b97a90e1f224ffd1af1d672007cea72221fc18955550051e&#39;, &#39;next_node&#39;: &lt;__main__.Node object at 0x108c045e0&gt;} . We now have officially created the most basic blockchain! Now, you can still change the value of Node.data. But that would make the next node&#39;s Node.prevous_node_hash incorrect. So, you would have to recompute the hash of the edited node and update the next node. However, now the hash in the following node does not match the one after that. You would have to change all the hashes until you arrive at the last node. In the case of crypto currencies, imagine that it takes monstrous amount of computation to add just a single node to the list and you start to realize why it is infeasible to edit the record. While you are changing the record and recomputing the whole chain, hundreds of thousands of computers are trying to add new nodes to the end of chain. You would never catch up. . Bitcoin and PyChain split the information in the node (from now on referred to as block) into two sections: a header and a body. This is done in order to save space and computation. For a lot of operations, you do not need the body; you just need a hash of it in order to validate the data should you want to. . Since PyChain is a general purpose blockchain, it can also discard some header fields that are present in bitcoin&#39;s header. For example: version, difficulty target and nonce. All of these are application specific and are important for bitcoin, but not to PyChain. Perhaps a field could be added to PyChain&#39;s header that could be used to store some information. . Implementing PyChain . PyChain is extremely simple, let&#39;s implement part of it: . Block . A block is defined as follows: . ======HEADER========== 4 bytes (I): block number : 0:4 32 bytes (32s): previous block hash: 4:36 32 bytes (32s): block body hash : 36:68 8 bytes (Q): block creation time: 68:76 ======END HEADER====== ======BODY============ n bytes (ns): block body : 76:n ======END BODY======== . The index of the block in the chain, the previous block&#39;s hash, the hash of the body, the time when the block was created and the body. The header is of fixed size, always 76 bytes. However, the body has a variable size, but is easy to compute: len(block)-76. . PyChain uses struct in order to encode and decode blocks. . import struct # a library to create C-style structs def encode_block(n: int, prev_hash: bytes, time: int, body: str): &quot;&quot;&quot; Encode a block Packs the block into an array of bytes :param n: block number :param prev_hash: previous block hash :param time: time of block creation :param body: block body &quot;&quot;&quot; block = struct.pack(f&quot;=I32s32sQ{len(body)}s&quot;, n, prev_hash, sha256(body.encode()).digest(), time, body.encode()) return block def decode_block(block: bytes): &quot;&quot;&quot; Decode a block Unpacks the block into an array of bytes Note: the 76 in the format string is the length of the header :param block: block to decode &quot;&quot;&quot; n, prev_hash, body_hash, time, body = struct.unpack( f&quot;=I32s32sQ{len(block)-76}s&quot;, block) return n, prev_hash, body_hash, time, body.decode() . These functions basically just take several values and either packs or unpacks them in order to achieve a C-like space efficiency. Easy as that. If you are unfamiliar with this library this is how it works: . import struct print(struct.pack(&quot;2I&quot;, 123, 456)) . b&#39;{ x00 x00 x00 xc8 x01 x00 x00&#39; . print(struct.unpack(&quot;2I&quot;, b&#39;{ x00 x00 x00 xc8 x01 x00 x00&#39;)) . (123, 456) . The format of a block is I32s32sQ{len(body)}s. Where Is are integers, ss are char arrays and Qs are unsigned longs. The number before the letter specifies how many of that type there are. Since the body has a variable number of s, it can be calculated using {len(block)-76}s. . To add a block to the chain we simply: . class BlockChain: blocks=[] HEADER_SLICE = slice(0, 76) def add_block(self, body: str): if len(blocks)==0: raise Exception(&quot;No genesis block&quot;) prev_header = self.blocks[-1][self.HEADER_SLICE] # Get the header of the previous block prev_hash = sha256(prev_header).digest() block = self.encode_block( len(self.blocks), prev_hash, int(time()), body) self.blocks.append(block) . Performance of PyChain . Speed . Since adding a block to the chain simply requires you to compute one hash and pack the data it is extremely fast. It takes only 24ms to add 10K blocks to the chain. That is 2.352 microseconds per block. The reason distributed blockchains take so long (approx. 10min for bitcoin) is because consensus is needed. In bitcoins case, to add one block hundreds of thousands of computers need to find a value that results in the header&#39;s hash starting with n number of zeros. The number of calculations needed to do this are astronomical. . Space . A blockchain containing 10K blocks (with empty bodies) would be 10000 blocks x 76 bytes = 760 KB. This is slightly better than bitcoin which has a header of 80 bytes. . Future development . PyChain needs some unit tests and more chain validation features in order to move to the next step. . The following step is to create a peer-to-peer protocol that would enable this blockchain implementation to be distributed. Of the top of my head, that would require the following abilities: . broadcast new block | get blocks (ask for blocks from peers) | . Appendix A . Fun fact! The current version of PyChain can be minified and fit in 31 unreadable lines of code: . O, N, M, L, H, B, C = round, staticmethod, isinstance, range, print, slice, len from hashlib import sha256 as E from time import time as G import struct as D from typing import List class K: blocks=[];HEADER_SLICE=B(4,80);BLOCK_NR_SLICE=B(4,8);PREV_HASH_SLICE=B(8,40);BODY_HASH_SLICE=B(40,72);BLOCK_CREATION_TIME_SLICE=B(72,80);BODY_SLICE=B(80,None) def add_block(A,body):B=A.blocks[-1][A.HEADER_SLICE];D=E(B).digest();F=A.encode_block(C(A.blocks),D,int(G()),body);A.blocks.append(F) def verify_chain(A): P=&#39;=Q&#39;;O=True;F=False if C(A.blocks)==0:return O for B in L(1,C(A.blocks)): G=E(A.blocks[B-1][A.HEADER_SLICE]).digest();H=A.blocks[B][A.PREV_HASH_SLICE] if G!=H:return F I=A.blocks[B][A.BODY_HASH_SLICE];J=E(A.blocks[B][A.BODY_SLICE]).digest() if I!=J:return F K=D.unpack(&#39;=I&#39;,A.blocks[B][A.BLOCK_NR_SLICE])[0] if K!=B:return F M=D.unpack(P,A.blocks[B-1][A.BLOCK_CREATION_TIME_SLICE])[0];N=D.unpack(P,A.blocks[B][A.BLOCK_CREATION_TIME_SLICE])[0] if N&lt;M:return F return O def import_chain(B,chain): A=chain if M(A,list):B.blocks=A elif M(A,bytes):B.blocks=[A] else:raise Exception(&#39;Invalid import data&#39;) def export_chain(A):return A.blocks @N def encode_block(n,prev_hash,time,body):A=body;B=D.pack(f&quot;=2I32s32sQ{C(A)}s&quot;,C(A),n,prev_hash,E(A.encode()).digest(),time,A.encode());return B @N def decode_block(block):A=block;B,E,F,G,H,I=D.unpack(f&quot;=2I32s32sQ{C(A)-80}s&quot;,A);return B,E,F,G,H,I.decode() .",
            "url": "https://nicolo.io/computer%20science/blockchain/2022/01/16/blockchains.html",
            "relUrl": "/computer%20science/blockchain/2022/01/16/blockchains.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Finding Support and Resitance levels",
            "content": "Support and resitance levels (SRL) are important price levels at which the price of a stock might bounce or in some cases penetrate leading to a change in sentiment that may fuel a new trend. SRLs can be horizontal, but may also be diagonal. The latter are also known as trendlines. These will be investigated in the next post. Finding horizontal SRLs is usually done manually by finding price reversal points and then connecting these points using a line. Humans are quite good at drawing lines connecting points as we can see what points are important and which ones are not. Machines have a more difficult time understanding this. . After identification, SRLs may be used in trading in a number of different ways. My favorites are &quot;break out&quot; and &quot;bounce&quot; strategies. The first one buys a stock when the price penetrates an important level. While the second one bets on the price level not being penetrated as the stock bounces on the price level. . In this post, I will evaluate three different techniques that can be used for finding SRLs. These techniques are: . Volume Profile | Counting | K-Means | . But first, as always, some importing of libraries. . import pymongo from tqdm import tqdm import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.pylab as pylab; pylab.rcParams[&#39;figure.figsize&#39;] = 20, 15 import mplfinance as mpf from scipy.signal import argrelextrema import pandas_ta as ta from collections import Counter from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score . Import data . Now let&#39;s import the data from the database and plot the last stock. . client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) daily = client.prod.daily symbols = [stock[&quot;yahoo&quot;] for stock in client.prod.stock_info.find()] all_dfs = dict() for symbol in tqdm(symbols[:10]): df = pd.DataFrame(daily.find({&quot;symbol&quot;:symbols[1]}).sort(&quot;date&quot;, pymongo.ASCENDING)) df.set_index(&quot;date&quot;, inplace=True) all_dfs[symbol] = df mpf.plot(df[:200], type=&#39;candle&#39;) . 100%|██████████| 10/10 [00:14&lt;00:00, 1.49s/it] . Argrelextrema . First, in order to find SRLs reversal points need to be found. There are many ways of doing this. Popular methods include the zig-zag indiator or argrelextrema. The latter will be used in this post. This algorithms deems a point to be a reversal if it is the greatest (or smallest) point withing a window. Essentially, the algorithm compares n points before and after the current point. . The following is what we want, these are reversal points: . df[:200].close.iloc[argrelextrema(df[:200].close.values, np.greater_equal, order=4)[0]].head() . date 2001-07-03 129.2140 2001-07-13 116.4473 2001-07-19 116.4473 2001-08-02 93.2352 2001-08-14 94.3958 Name: close, dtype: float64 . Notice that argrelextrama has a parameter called &quot;order&quot;. This integer indicates how many points around the current point that should be checked. If this number is large, then only &quot;large reversals&quot; will be found. The opposite is true for small values of &quot;order&quot;. Larger values result in fewer reversals. . Now let&#39;s plot these reversals together with the stock price. The reversals will be found with order sizes of 4 and 10 respecively. . view = df[:200] fig, axs = plt.subplots(1, 2, figsize=(10, 5)) for ax, order in zip(axs, [4, 10]): peaks = view.close.iloc[argrelextrema(view.close.values, np.greater_equal, order=order)[0]] bottoms = view.close.iloc[argrelextrema(view.close.values, np.less_equal, order=order)[0]] ax.plot(view.close) ax.scatter(peaks.index, peaks, color=&quot;g&quot;) ax.scatter(bottoms.index, bottoms, color=&quot;r&quot;) for tick in ax.get_xticklabels(): tick.set_rotation(90) plt.show() . As it can be seen, the algorithm finds the peaks and bottoms of the stock price. Notice that around January 2002 the algorithm found multiple tops after each other. If the zig-zag indicator would have been used, then this output would not have been seen. The indicator requires there to be a bottom before a new top. . Note: notice that the point around October 2001 make up a head and should formation. In the coming posts these points will be used to find price action formations such as VCP. . Getting all the pivots . Just getting reversals for the close price can be useful, but the price might not close at the SRL. Sometimes the touch of the SRL is at the high or low of the day. Therefore, reversals for both the high, low and close have to be found. This is done using the get_pivots() function. A list of all prices where the stock reverses is returned. As the price of a stock is quite noisy, the prices are rounded to the closest integer. . The aforementioned function works well, but can be improved by incorporating different values for order. I chose to call this function multi_scale_pivots(). One feature of this function is that it weights larger values for order more. This is good as larger reversals are more important than smaller ones. The function does this by featuring the prices of bigger reversals more often (get_pivots(df, order=n)*(n-min(times))). . def get_pivots(df, order=4): pivots = list() pivots += list(df.low.iloc[argrelextrema(df.low.values, np.less_equal, order=order)[0]].values) pivots += list(df.high.iloc[argrelextrema(df.high.values, np.greater_equal, order=order)[0]].values) pivots += list(df.close.iloc[argrelextrema(df.close.values, np.less_equal, order=order)[0]].values) pivots += list(df.close.iloc[argrelextrema(df.close.values, np.greater_equal, order=order)[0]].values) pivots = [round(pivot) for pivot in pivots] return pivots def multi_scale_pivots(df, times=range(4, 10)): pivots = list() for n in times: pivots+=get_pivots(df, order=n)*(n-min(times)) return pivots print(&quot;get_pivots() - length:&quot;, len(get_pivots(df[:20]))) print(get_pivots(df[:20])) print(&quot; nmulti_scale_pivots() - length:&quot;, len(multi_scale_pivots(df[:20]))) print(multi_scale_pivots(df[:20])) . get_pivots() - length: 12 [115, 107, 92, 130, 130, 118, 118, 111, 93, 129, 116, 116] multi_scale_pivots() - length: 118 [115, 107, 92, 130, 130, 118, 118, 111, 93, 129, 116, 116, 115, 107, 92, 130, 130, 118, 111, 93, 129, 116, 115, 107, 92, 130, 130, 118, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116] . The following function has been created to plot the result of the models: . def plot_SR(df, func, size = 150): fig, axs = plt.subplots(2,2, figsize=(10, 10)) axs = axs.reshape(-1) for i, ax in enumerate(axs): view = df[size*i:size*(1+i)] sr = func(view) mpf.plot(view, ax = ax, type=&quot;candle&quot;, hlines={&quot;hlines&quot;:list(sr), &quot;alpha&quot;:0.3, &quot;colors&quot;:[&quot;r&quot;, &quot;g&quot;, &quot;b&quot;, &quot;c&quot;, &quot;m&quot;, &quot;y&quot;]}) for tick in ax.get_xticklabels(): tick.set_rotation(90) plt.show() . Volume Profile . The first model that is going to be used is based on the volume profile and has therefore been named such. The &quot;volume&quot; is the number of shares traded on a specific time interval (e.g. per day). See the volume column in the cell below. The volume profile measures the number of shares traded in a price interval. Large volumes at a specific price can indicate SRLs. One explanation might be that during a downward trend seller sell at lower and lower prices. At some point they are not willing to sell for less per share and buyers are stepping in and absorbing the sell pressure. It might take a lot of buying before the sentiment changes and the price changes direction. Therefore, quite some volume might be traded at the reversal price. At a future time, sellers might become buyers are prices which have previously acted as reversal zones. Thusly, even more volume is traded at that price. . The way that this technique is going to be applied is by calculating the volume profile and then choosing the n price levels with the highest volume. . df[[&quot;open&quot;, &quot;high&quot;, &quot;low&quot;, &quot;close&quot;, &quot;volume&quot;]].head(1) . open high low close volume . date . 2001-06-27 117.9948 | 119.5423 | 115.2867 | 117.6079 | 1088873 | . def volume_profile(view, bins = 10, n = 4): vp = view.copy().ta.vp(bins) sr = vp.iloc[np.argsort(vp[&quot;total_volume&quot;])[:n]][&quot;mean_close&quot;] return list(sr) plot_SR(df, volume_profile) . As it can be seen in the plots above, the levels chosen by the algorithm are not always exactly at the reversals. This can be explained by the fact that a lot of volume is also traded in consolidation. Sometimes more than at the reversals. These SRLs are more suitable as reversal zones rather than reversal prices. . K-Means . The second technique that is going to be evaluated will be named k-means and it uses machine learning to find the center of a cluster of points (for illustration see video by Najam Syed). In this case, the points are going to be prices and the centers will be the support and resitance points. This approach was inspired by Karl Tengelin in his master thesis in 2020. The difference between the following approach and Tengelin&#39;s approach is that he used tick data and in this case daily data will be used. . Following Tengelin&#39;s approach, all closing prices are given to the model. The model then tries to find the centers of what hopefully are clusters. This ML model&#39;s primary parameter is k. The value chosen for this parameter should be chosen such that the silloute score is maximised. The code below was heavily inspired by Suhail Saqan&#39;s blog post. . def k_means_all(view): def optimum_Kvalue(data): kmax = 11 sil = {} k_model = {} for k in range(2, kmax+1): kmeans = KMeans(n_clusters = k).fit(data) k_model[k] = kmeans labels = kmeans.labels_ sil[k]=(silhouette_score(data, labels)) optimum_cluster = k_model[max(sil, key=sil.get)] return optimum_cluster model = optimum_Kvalue(np.array(view.close).reshape(-1, 1)) return list(model.cluster_centers_.reshape(-1)) plot_SR(df, k_means_all) . The plots above do not look very good. Some have a lot of lines and some have very few. Very few are at SRLs. . One thing that can be optimized is to feed the model the reversals prices and make it cluster these. . def k_means_reversals(view): def optimum_Kvalue(data): kmax = 11 sil = {} k_model = {} for k in range(2, kmax+1): kmeans = KMeans(n_clusters = k).fit(data) k_model[k] = kmeans labels = kmeans.labels_ sil[k]=(silhouette_score(data, labels)) optimum_cluster = k_model[max(sil, key=sil.get)] return optimum_cluster model = optimum_Kvalue(np.array(multi_scale_pivots(view)).reshape(-1, 1)) return list(model.cluster_centers_.reshape(-1)) plot_SR(df, k_means_reversals) . These results are far better. The model is able to find good SRLs. One drawdown of this model is that the user is not able to tell it how many levels to find. By lowering the value of k the performance of the model may be impacted. . Count . The final method has been named count, because that is what it does. The number of time the price reverses at a price is counted. The higher count the better. This means that the user can chose to plot the n best lines by ranking the lines by count. . The following code is used to count: . def count(view, n = 7): pivots = multi_scale_pivots(view) count = Counter(pivots) count = {k: v for k, v in sorted(count.items(), key=lambda item: -item[1])} return list(count.keys())[:n] plot_SR(df, count) . This method performes on par with k-means, but has the advantage of ranking the results. . Comparison . Now the performance of the three methods will be compared. The model&#39;s outputs will be plotted together in order to see the difference. However, the figures can seem a little messy as there may in some places be a few lines in very close proximity. . These are the colors: . Volume Profile: RED | K-means: GREEN | Count: BLUE | . fig, axs = plt.subplots(2,2, figsize=(15, 15)) axs = axs.reshape(-1) for i, ax in enumerate(axs): view = df[150*i:150*(1+i)] vp = list(volume_profile(view)) km = list(k_means_reversals(view)) cn = list(count(view)) mpf.plot(view, ax = ax, type=&quot;candle&quot;, hlines={&quot;hlines&quot;:vp+km+cn, &quot;alpha&quot;:0.5, &quot;colors&quot;:[&quot;r&quot;]*len(vp) + [&quot;g&quot;]*len(km) + [&quot;b&quot;]*len(cn)}) plt.show() . In my opinion, the k-means&#39; SRLs are the best. The problem with the number of lines found can be solved by combining the k-means models with the count model. This is done by finding labels of all the samples and then sorting the labels from most to least frequently used. . This is the final result: . from collections import Counter, defaultdict def k_means_reversals_count(view, n=6): def optimum_Kvalue(data): kmax = 11 sil = {} k_model = {} for k in range(2, kmax+1): kmeans = KMeans(n_clusters = k).fit(data) k_model[k] = kmeans labels = kmeans.labels_ sil[k]=(silhouette_score(data, labels)) optimum_cluster = k_model[max(sil, key=sil.get)] return optimum_cluster model = optimum_Kvalue(np.array(multi_scale_pivots(view)).reshape(-1, 1)) clusters = model.cluster_centers_.reshape(-1) d = dict() for label, count in Counter(model.labels_).items(): d[clusters[label]] = count d = {k: v for k, v in sorted(d.items(), key=lambda item: -item[1])} return list(d.keys())[:n] plot_SR(df, k_means_reversals_count) . In the next post trendline finders will be created! .",
            "url": "https://nicolo.io/finance/analysis/2021/07/09/Support-Resistance.html",
            "relUrl": "/finance/analysis/2021/07/09/Support-Resistance.html",
            "date": " • Jul 9, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Downloading daily stock data",
            "content": "My ambition is to build a fully automated trading system that: . Downloads the latest data | Performs some cleaning steps | Generates trading signals | Opens and closes positions in accordance with a risk management framework | . This system will be built step by step in a series of posts. As the list suggests, the first step is to update the database with the latest data when it is available. First, what database are we using? . Database . There are many options when it comes to chosing a database. The most popular type of database for tabular data is a SQL database. This is due to their good performance and data structure. SQL databases are structured in a tabular fasion which is the same structure that pandas dataframes are structured. Noteworthy SQL databases are MariaDB, PostgreSQL and SQLite. However, a SQL database will not be used in this project for a number of reasons. Instead, a NOSQL database will be used. More specifically, MongoDB. But, why? First let&#39;s address the pros and cons of SQL databases. It is true that they usually have good performance. This can be a deciding factor, but for small amounts of data (&lt;50GB) there might not be any difference. NOSQL databases can be just as performant if queried correctly and if indicies are used. When it comes to the data structure, some restructuring of the data has to be performed before it can be inserted into a NOSQL database. But this is not a problem, many python libraries have functions such as dataframe.to_dict(&quot;records&quot;) that easily converts tabular data into a list of dictionaries suited for NOSQL databases. These arguments cancel eachother out, and either database type could be used. The final con of SQL databases that tips the needle in favor of NOSQL databases is that the python libraries that are used to access SQL databases have terrible APIs. Not only that, setting up such a database is also a painful process that takes a lot of time if it is to be done right. . Imports and connections . Now for the program used to download the data. The following imports and connections are going to be needed. . import yfinance as yf import pymongo from tqdm import tqdm from datetime import datetime import pandas as pd import borsdata_api as api api = api.BorsdataAPI(open(&quot;api.txt&quot;, &quot;r&quot;).read()) # Börsdata is used as data source client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) # MongoDB server connection daily = client.prod.daily # The collection used to store daily data . Defining aggregation query and getting the symbols . It is important not to add duplicate data to the database. This can be solved by quering the database for all the dates that are already present in the database and then ignore these dates when adding the new data. This can be done with the following MongoDB aggregation query. It essencially matches the wanted stock symbol and then creates a list with all the dates. This cell also contains the query for all the stock symbols. These are stored in a separate collection called stock_info. The list of symbols returned will be used to tell the API what stocks to download. . aggreg_query = [ { &#39;$match&#39;: {&#39;symbol&#39;: None} }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$symbol&#39;, &#39;index&#39;: {&#39;$push&#39;: &#39;$date&#39;} } } ] symbols = [(stock[&quot;yahoo&quot;], stock[&quot;insId&quot;]) for stock in client.prod.stock_info.find()] print(&quot;Number of symbols:&quot;, len(symbols)) . Number of symbols: 1780 . Download loop . This is where the magic happens. This for-loop loops through the list of symbols previously retrieved and performs these steps: . Downloads the data. | Removes rows with NaN values. | Detaches the index (dates) and adds it as a column. | Makes all the column names lowercase. | Adds the symbol to every row. This is an important field for when every row becomes a document. | Adds the date and time when the data was retrieved. | Adds a string as the primary key. This a second security measure to prevents the adding of the same date twice to the database. | Removes the already present rows from the data we are going to add to the database. This uses the abovementioned aggregation query. | Finally, it converts the tabular data to documents (dictionaries) and inserts them into the database. | . This loop may take a long time to finish. Especially when the number of stocks is large. Some APIs might stop giving you data by blocking your requests a long time before all the symbols have been looped through. This is one of the reasons by Börsdata is used. Although it is a payed service, it does not limit the number of requests. If this is not an option for you, YahooFinance&#39;s API can be used. The pros are that they have all the stocks you could ever want and are free, but they start blocking requests after a couple of hundred queries. . progress = tqdm(symbols) for symbol, insId in progress: progress.set_description(symbol) # Set progress bar description df = api.get_instrument_stock_prices(insId) # Download the data #df = data[symbol] df = df.dropna() df.reset_index(level=0, inplace=True) df.columns = [column.lower() for column in df.columns] df[&quot;symbol&quot;] = symbol df[&quot;updated&quot;] = datetime.now() df[&quot;_id&quot;] = df[&quot;symbol&quot;] + &quot;:&quot; + df[&quot;date&quot;].astype(str) aggreg_query[0][&quot;$match&quot;][&quot;symbol&quot;] = symbol index = list(daily.aggregate(aggreg_query)) if len(index)&gt;0: # Check if stock already in db index=index[0] index = pd.Series(index[&quot;index&quot;]) df = df[~df[&#39;date&#39;].isin(index)] # Ignoring indicies already in db if len(df)&gt;0: daily.insert_many(df.to_dict(&#39;records&#39;)) . Cronjob . To make the data retrieval automatic it has to be scheduled. This can easily be done using cronjob on linux machines. The following script will be made into a cronjob and this parameter will be added to the crontab 0 18 * * 1-5 data_downloader/daily_downloader.py. The parameter means that the command should be run every weekday at 18 o&#39;clock. . #- #!data_downloader/env/bin/python import yfinance as yf import pymongo from tqdm import tqdm from datetime import datetime import pandas as pd import borsdata_api as api api = api.BorsdataAPI(open(&quot;api.txt&quot;, &quot;r&quot;).read()) # Börsdata is used as data source client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) # MongoDB server connection daily = client.prod.daily # The collection used to store daily data aggreg_query = [ { &#39;$match&#39;: {&#39;symbol&#39;: None} }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$symbol&#39;, &#39;index&#39;: {&#39;$push&#39;: &#39;$date&#39;} } } ] symbols = [(stock[&quot;yahoo&quot;], stock[&quot;insId&quot;]) for stock in client.prod.stock_info.find()] print(&quot;Number of symbols:&quot;, len(symbols)) progress = tqdm(symbols) for symbol, insId in progress: progress.set_description(symbol) # Set progress bar description df = api.get_instrument_stock_prices(insId) # Download the data #df = data[symbol] df = df.dropna() df.reset_index(level=0, inplace=True) df.columns = [column.lower() for column in df.columns] df[&quot;symbol&quot;] = symbol df[&quot;updated&quot;] = datetime.now() df[&quot;_id&quot;] = df[&quot;symbol&quot;] + &quot;:&quot; + df[&quot;date&quot;].astype(str) aggreg_query[0][&quot;$match&quot;][&quot;symbol&quot;] = symbol index = list(daily.aggregate(aggreg_query)) if len(index)&gt;0: # Check if stock already in db index=index[0] index = pd.Series(index[&quot;index&quot;]) df = df[~df[&#39;date&#39;].isin(index)] # Ignoring indicies already in db if len(df)&gt;0: daily.insert_many(df.to_dict(&#39;records&#39;)) .",
            "url": "https://nicolo.io/finance/data/2021/06/28/Data-Retrieval.html",
            "relUrl": "/finance/data/2021/06/28/Data-Retrieval.html",
            "date": " • Jun 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "The goal of this blog is to write about topics that I find interesting. Categories that will be in focus are computer science and finance, my two favorite domains. I like reading academical papers so I will sprinkle links to these in all relevant places. .",
          "url": "https://nicolo.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nicolo.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}